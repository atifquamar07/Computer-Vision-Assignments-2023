# -*- coding: utf-8 -*-
"""2020523-MOT_SORT_HW4_Workbook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kiT1rRuq2DhzHX4D1WiqB46Dn53yOPpv

## **CV HW4: Multi-object Tracking (MOT) with Detection**
**Detection**: YOLOv5, 
**Tracking**: Simple Online Realtime Tracking (SORT)

---

## **1. Unzip data folder**
"""

# Change the path according to your setup 
from google.colab import drive
drive.mount("/content/gdrive")
!unzip "/content/gdrive/MyDrive/HW4_Resources/sort-master"
!unzip "/content/gdrive/MyDrive/HW4_Resources/KITTI_17_images"

"""# **2. Install requirements**"""

# Commented out IPython magic to ensure Python compatibility.
# %cd pytorch_objectdetecttrack
!pip install filterpy==1.1.0

"""# **3. Import libraries**"""

import torch
import torchvision
import cv2
import sys
import os
sys.path.insert(0,'./sort-master/')
import matplotlib
from google.colab.patches import cv2_imshow
from collections import namedtuple, OrderedDict

"""# **4. Load YOLOv5 detector from torch hub**"""

yolov5_detector = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained = True)
yolov5_detector.float()
yolov5_detector.eval()

"""# **5. Import SORT library**"""

from sort import *

"""#**6. Perform tracking with detection**"""

# Write your code here to perform tracking with detection using the provided YOLOv5 model and the SORT implementation
model = torch.hub.load("ultralytics/yolov5", "yolov5s", pretrained = True)
model.float()
model.eval()

"""# **6.1. Generating Video from the KITTI_17_images dataset**

"""

data = "KITTI_17_images"
video = "ImageToVideo.mp4"

img_files = sorted(os.listdir(data))

fps = 30.0
frame_size = cv2.imread(os.path.join(data, img_files[0])).shape[:2][::-1]
fourcc = cv2.VideoWriter_fourcc(*'mp4v')

out = cv2.VideoWriter(video, fourcc, fps, frame_size)

for img_file in img_files:
    img_path = os.path.join(data, img_file)
    img = cv2.imread(img_path)
    out.write(img)

out.release()
cv2.destroyAllWindows()

"""# **6.2. Implementing object tracking on the generated video**"""

vid = cv2.VideoCapture(video)
mot_tracker = Sort()
colours = [(0, 255, 0), (255, 255, 0), (255, 0, 0), (0, 0, 255)]

# Initialize video writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
fps = 30.0
frame_size = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))
out = cv2.VideoWriter('outputVideoFile.mp4', fourcc, fps, frame_size)

# Reading the first frame of input video
ret, frame = vid.read()
frame_counter = 1
with open('gt.txt', 'w') as file: 
    while True:
        if not ret:  # No more frames to read
            break
        # Generating predictions
        predictions = model(frame)
        # Loading detections
        detections = predictions.pred[0].numpy()
        # Filtering only person object detection
        person_detections = detections[detections[:, 5] == 0]

        retMotTracker = mot_tracker.update(person_detections).tolist()

        # Generating the ground truth value for this frame
        for id in range(len(retMotTracker)):
            x1, y1, w, h, score = retMotTracker[id]
            id = id + 1  
            file.write(f"{frame_counter},{id},{x1},{y1},{w},{h},{score},-1,-1,-1\n")

        # Making rectangles on the detected persons
        for j in range(len(retMotTracker)):
            idName = "ID : " + str(int(retMotTracker[j][4]))
            cv2.rectangle(frame, (int(retMotTracker[j][0]), int(retMotTracker[j][1])), (int(retMotTracker[j][2]), int(retMotTracker[j][3])), colours[int(retMotTracker[j][4])%4], 2)
            cv2.putText(frame, idName, (int(retMotTracker[j][0]), int(retMotTracker[j][1])-10), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 0.9, colours[int(retMotTracker[j][4])%4], 2)

        # Write image frame to output video
        out.write(frame)
        frame_counter += 1

        # Read next frame of input video
        ret, frame = vid.read()

# Release video writer
out.release()
vid.release()

"""# **7. Report Evaluation Metrics**"""

# Use the Track-Eval kit to report the complete set of performance and accuracy metrics
# Comment on and interpret MOTA and MOTP values
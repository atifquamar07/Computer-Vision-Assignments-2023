{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL39SP50yvLP",
        "outputId": "362a2545-433e-436b-9a7d-a78dfda5cf8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.10-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (4.5.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.15.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 KB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=7192716cd4c51e68d83bb907fc83f1ebe8c48ea77f40aec8e2da08e452f0d4da\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, urllib3, smmap, setproctitle, docker-pycreds, sentry-sdk, gitdb, GitPython, wandb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.15.0 setproctitle-1.3.2 smmap-5.0.0 urllib3-1.26.14 wandb-0.13.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbzCSf_dyFmA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import pickle\n",
        "from sklearn import manifold\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report\n",
        "import wandb\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "zip_path = \"/content/gdrive/MyDrive/svhn_yolo.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1PjekOQyrZz",
        "outputId": "402a9236-d426-4f21-9c40-ff7d3124e020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Open the .zip file\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    # Extract all the contents of the .zip file\n",
        "    zip_ref.extractall(\"SVHN_YOLO\")"
      ],
      "metadata": {
        "id": "3N128bPuzXTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the main folder path\n",
        "main_folder_path = \"svhn_folder\"\n",
        "\n",
        "# Define the subfolder paths\n",
        "images_train_path = os.path.join(main_folder_path, \"images\", \"train\")\n",
        "images_val_path = os.path.join(main_folder_path, \"images\", \"val\")\n",
        "labels_train_path = os.path.join(main_folder_path, \"labels\", \"train\")\n",
        "labels_val_path = os.path.join(main_folder_path, \"labels\", \"val\")\n",
        "\n",
        "# Create the directory structure\n",
        "os.makedirs(images_train_path, exist_ok=True)\n",
        "os.makedirs(images_val_path, exist_ok=True)\n",
        "os.makedirs(labels_train_path, exist_ok=True)\n",
        "os.makedirs(labels_val_path, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "nCpzUYA40iKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install tree\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hvPs2ie0sMo",
        "outputId": "e0d3f723-5b0d-4dda-9f43-10adf6936961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [1\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [W\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [W\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "\r0% [Waiting for headers] [3 InRelease 14.2 kB/114 kB 12%] [Waiting for headers]\r                                                                               \rGet:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
            "\r0% [Waiting for headers] [3 InRelease 14.2 kB/114 kB 12%] [Waiting for headers]\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:14 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2,970 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,386 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,131 kB]\n",
            "Fetched 6,844 kB in 3s (2,598 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 43.0 kB of archives.\n",
            "After this operation, 115 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tree amd64 1.8.0-1 [43.0 kB]\n",
            "Fetched 43.0 kB in 0s (129 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 128126 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.8.0-1_amd64.deb ...\n",
            "Unpacking tree (1.8.0-1) ...\n",
            "Setting up tree (1.8.0-1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tree svhn_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L94hQ40p0yG6",
        "outputId": "f90dd44b-f55b-456e-bbf1-aeadc1db2fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34msvhn_folder\u001b[00m\n",
            "├── \u001b[01;34mimages\u001b[00m\n",
            "│   ├── \u001b[01;34mtrain\u001b[00m\n",
            "│   └── \u001b[01;34mval\u001b[00m\n",
            "└── \u001b[01;34mlabels\u001b[00m\n",
            "    ├── \u001b[01;34mtrain\u001b[00m\n",
            "    └── \u001b[01;34mval\u001b[00m\n",
            "\n",
            "6 directories, 0 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# Define the folder path\n",
        "images_path = \"SVHN_YOLO/svhn/images/all\"\n",
        "labels_path = \"SVHN_YOLO/svhn/labels/all\"\n",
        "\n",
        "# Get the names of all the \".png\" files in the folder\n",
        "image_files = glob.glob(os.path.join(images_path, \"*.png\"))\n",
        "label_files = glob.glob(os.path.join(labels_path, \"*.txt\"))\n",
        "\n",
        "# Extract the file names from the file paths\n",
        "images_list = [os.path.basename(file_path) for file_path in image_files]\n",
        "labels_list = [os.path.basename(file_path) for file_path in label_files]\n",
        "\n",
        "print(len(images_list))\n",
        "print(len(labels_list))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgvAcYXk1kca",
        "outputId": "18c5fb35-11f4-4ee1-e716-427deae3a598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33402\n",
            "33402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = len(images_list)"
      ],
      "metadata": {
        "id": "W4TyG30L3m87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training, validation, and testing sets\n",
        "val_split = int(0.2 * num_samples)\n",
        "test_split = int(0.1 * num_samples) + val_split"
      ],
      "metadata": {
        "id": "qkN7lVIx3fZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validation data splitted to 20% of the dataset\n",
        "images_val_data = images_list[:val_split]  \n",
        "labels_val_data = labels_list[:val_split]\n",
        "\n",
        "# testing data splitted to 10% of the dataset\n",
        "images_test_data = images_list[val_split:test_split]\n",
        "labels_test_data = labels_list[val_split:test_split]\n",
        "\n",
        "# rest reserved for training data\n",
        "images_train_data = images_list[test_split:]\n",
        "labels_train_data = labels_list[test_split:]"
      ],
      "metadata": {
        "id": "Bey7BThS3lFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "src_folder1 = \"SVHN_YOLO/svhn/images/all\"\n",
        "dst_folder1 = \"svhn_folder/images/train\"\n",
        "\n",
        "for image_name in images_train_data:\n",
        "    src_file = os.path.join(src_folder1, image_name)\n",
        "    dst_file = os.path.join(dst_folder1, image_name)\n",
        "    shutil.copy(src_file, dst_file)\n",
        "\n",
        "src_folder2 = \"SVHN_YOLO/svhn/images/all\"\n",
        "dst_folder2 = \"svhn_folder/images/val\"\n",
        "\n",
        "for image_name in images_val_data:\n",
        "    src_file = os.path.join(src_folder2, image_name)\n",
        "    dst_file = os.path.join(dst_folder2, image_name)\n",
        "    shutil.copy(src_file, dst_file)\n",
        "\n",
        "src_folder3 = \"SVHN_YOLO/svhn/labels/all\"\n",
        "dst_folder3 = \"svhn_folder/labels/train\"\n",
        "\n",
        "for image_name in labels_train_data:\n",
        "    src_file = os.path.join(src_folder3, image_name)\n",
        "    dst_file = os.path.join(dst_folder3, image_name)\n",
        "    shutil.copy(src_file, dst_file)\n",
        "\n",
        "\n",
        "src_folder4 = \"SVHN_YOLO/svhn/labels/all\"\n",
        "dst_folder4 = \"svhn_folder/labels/val\"\n",
        "\n",
        "for image_name in labels_val_data:\n",
        "    src_file = os.path.join(src_folder4, image_name)\n",
        "    dst_file = os.path.join(dst_folder4, image_name)\n",
        "    shutil.copy(src_file, dst_file)\n"
      ],
      "metadata": {
        "id": "dYYmqc0D60eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SVHN_YOLO_Dataset(Dataset):\n",
        "    def __init__(self, img_file, lab_file, transform=None):\n",
        "        self.image_files = img_file\n",
        "        self.label_files = lab_file\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self):\n",
        "        return self.image_files, self.label_files\n",
        "\n",
        "train_set = SVHN_YOLO_Dataset(images_train_data, labels_train_data)\n",
        "test_set = SVHN_YOLO_Dataset(images_test_data, labels_test_data)\n",
        "val_set = SVHN_YOLO_Dataset(images_val_data, labels_val_data)\n",
        "\n",
        "# Define the data loaders for each split\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(val_set, batch_size=100, shuffle=True)"
      ],
      "metadata": {
        "id": "ZB4gLiI752mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8gwGzkt8WJ8",
        "outputId": "6a1ca265-fb2c-425e-f624-e9915d444635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 15232, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 15232 (delta 5), reused 10 (delta 4), pack-reused 15218\u001b[K\n",
            "Receiving objects: 100% (15232/15232), 14.17 MiB | 32.10 MiB/s, done.\n",
            "Resolving deltas: 100% (10441/10441), done.\n",
            "/content/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 256 --cfg yolov5x.yaml --batch 10 --epochs 10 --data svhn.yaml --weights yolov5x.pt --workers 24 --name cv_q3_yolo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqfZkPfc-MHt",
        "outputId": "352bfa74-1276-4162-f8c6-2ae1e99dc54b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ⚠️ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5x.pt, cfg=yolov5x.yaml, data=svhn.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=10, imgsz=256, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=24, project=runs/train, name=cv_q3_yolo, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-107-g7a972e8 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "2023-02-19 19:14:56.171500: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-19 19:14:57.135119: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-02-19 19:14:57.135267: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-02-19 19:14:57.135289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      8800  models.common.Conv                      [3, 80, 6, 2, 2]              \n",
            "  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n",
            "  2                -1  4    309120  models.common.C3                        [160, 160, 4]                 \n",
            "  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n",
            "  4                -1  8   2259200  models.common.C3                        [320, 320, 8]                 \n",
            "  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n",
            "  6                -1 12  13125120  models.common.C3                        [640, 640, 12]                \n",
            "  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n",
            "  8                -1  4  19676160  models.common.C3                        [1280, 1280, 4]               \n",
            "  9                -1  1   4099840  models.common.SPPF                      [1280, 1280, 5]               \n",
            " 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  4   5332480  models.common.C3                        [1280, 640, 4, False]         \n",
            " 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  4   1335040  models.common.C3                        [640, 320, 4, False]          \n",
            " 18                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  4   4922880  models.common.C3                        [640, 640, 4, False]          \n",
            " 21                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  4  19676160  models.common.C3                        [1280, 1280, 4, False]        \n",
            " 24      [17, 20, 23]  1    100935  models.yolo.Detect                      [10, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\n",
            "YOLOv5x summary: 445 layers, 86278375 parameters, 86278375 gradients, 204.8 GFLOPs\n",
            "\n",
            "Transferred 738/745 items from yolov5x.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 123 weight(decay=0.0), 126 weight(decay=0.00046875), 126 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/svhn_folder/labels/train.cache... 16388 images, 6994 backgrounds, 0 corrupt: 100% 23382/23382 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/svhn_folder/labels/val.cache... 1308 images, 5372 backgrounds, 0 corrupt: 100% 6680/6680 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.86 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/cv_q3_yolo2/labels.jpg... \n",
            "Image sizes 256 train, 256 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/cv_q3_yolo2\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/9      2.93G    0.05934    0.02021    0.05341         15        256: 100% 2339/2339 [10:05<00:00,  3.86it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 334/334 [00:41<00:00,  8.08it/s]\n",
            "                   all       6680       2817      0.138      0.608      0.142     0.0657\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/9      3.28G    0.04945    0.01721    0.02669         10        256: 100% 2339/2339 [09:09<00:00,  4.26it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 334/334 [00:39<00:00,  8.45it/s]\n",
            "                   all       6680       2817      0.176      0.751      0.173     0.0839\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        2/9      3.28G    0.04785    0.01734    0.02253          5        256: 100% 2339/2339 [08:56<00:00,  4.36it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 334/334 [00:40<00:00,  8.32it/s]\n",
            "                   all       6680       2817      0.177      0.767      0.175     0.0839\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        3/9      3.28G    0.04645     0.0172     0.0211         35        256:  92% 2143/2339 [08:09<00:44,  4.38it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 640, in <module>\n",
            "    main(opt)\n",
            "  File \"train.py\", line 529, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"train.py\", line 311, in train\n",
            "    loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size\n",
            "  File \"/content/yolov5/utils/loss.py\", line 125, in __call__\n",
            "    tcls, tbox, indices, anchors = self.build_targets(p, targets)  # targets\n",
            "  File \"/content/yolov5/utils/loss.py\", line 217, in build_targets\n",
            "    offsets = (torch.zeros_like(gxy)[None] + off[:, None])[j]\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WT_JcgRT-ork"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Techniques used for Data Augmentation are Random Cropping, Random Erasing and Random Rotation."
      ],
      "metadata": {
        "id": "B9WyMkaSIiLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Random Cropping"
      ],
      "metadata": {
        "id": "PztMiYUCy4nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_crop(img, sz):\n",
        "    # height, width = img.shape[:2]\n",
        "    h = img.shape[0]\n",
        "    w = img.shape[1]\n",
        "    ch, cw = sz\n",
        "\n",
        "    y = np.random.randint(0, h - ch + 1)\n",
        "    x = np.random.randint(0, w - cw + 1)\n",
        "\n",
        "    return img[y:y+ch, x:x+cw, :]\n"
      ],
      "metadata": {
        "id": "0vg3fAwQXq4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Erasing\n"
      ],
      "metadata": {
        "id": "fclrEp-08pqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_erasing(img):\n",
        "\n",
        "    if np.random.uniform() > 0.5:\n",
        "        return img\n",
        "\n",
        "    h = img.shape[0]\n",
        "    w = img.shape[1]\n",
        "    c = img.shape[2]\n",
        "\n",
        "    # h, w, c = img.shape\n",
        "    area = h * w\n",
        "\n",
        "    while True:\n",
        "        target_area = np.random.uniform(0.02, 0.4) * area\n",
        "        aspect_ratio = np.random.uniform(0.3, 3.3)\n",
        "        er_h = int(round(np.sqrt(target_area * aspect_ratio)))\n",
        "        er_w = int(round(np.sqrt(target_area / aspect_ratio)))\n",
        "        if er_h < h and er_w < w:\n",
        "            break\n",
        "\n",
        "    x1 = np.random.randint(0, h - er_h)\n",
        "    y1 = np.random.randint(0, w - er_w)\n",
        "\n",
        "    if c == 1:\n",
        "        img[x1:x1+er_h, y1:y1+er_w] = np.random.uniform(0, 1)\n",
        "    else:\n",
        "        for i in range(c):\n",
        "            img[x1:x1+er_h, y1:y1+er_w, i] = np.random.uniform(0, 1)\n",
        "\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "mZsWxBhA81sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Rotation"
      ],
      "metadata": {
        "id": "9WGnP1UCIZQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_rotation(img, angle_range=(-15, 15)):\n",
        "\n",
        "    # Generating a random rotation\n",
        "    angle = np.random.uniform(angle_range[0], angle_range[1])\n",
        "\n",
        "    # Image center\n",
        "    h, w = img.shape[:2]\n",
        "    cx, cy = w // 2, h // 2\n",
        "\n",
        "    # Rotation matrix\n",
        "    M = cv2.getRotationMatrix2D((cx, cy), angle, 1.0)\n",
        "\n",
        "    # Apply the rotation \n",
        "    rotated_img = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
        "\n",
        "    return rotated_img\n"
      ],
      "metadata": {
        "id": "7s2UEaPDHqKT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}